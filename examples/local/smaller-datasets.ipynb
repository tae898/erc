{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bitdevpython378e162af75d134820b03d49898b79756f",
   "display_name": "Python 3.7.9 64-bit ('dev-python-3.7')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "there are 9989 vids (utterances) in the original train dataset\n",
      "there are 1038 dialouges in the original train dataset\n",
      "\n",
      "789 utterances in the smaller-dataset\n"
     ]
    }
   ],
   "source": [
    "annotations_path = '/home/tk/repos/MELD/data/MELD/train_sent_emo.csv'\n",
    "vid_dir = \"/home/tk/datasets/MELD/MELD.Raw/train/train_splits\"\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import csv\n",
    "import shutil\n",
    "with open(annotations_path) as f:\n",
    "    reader = csv.reader(f)\n",
    "    annotations = list(reader)\n",
    "\n",
    "utts = glob(os.path.join(vid_dir, '*.mp4'))\n",
    "dias = sorted(list(set([u.split('_utt')[0] for u in utts])))\n",
    "for d in dias:\n",
    "    assert \"train_splits\" in d\n",
    "\n",
    "print(f\"there are {len(utts)} vids (utterances) in the original train dataset\")\n",
    "print(f\"there are {len(dias)} dialouges in the original train dataset\")\n",
    "print()\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "random.shuffle(dias)\n",
    "\n",
    "SMALL, MEDIUM, LARGE = 20, 40, 80\n",
    "dias_reduced = dias[:LARGE]\n",
    "utts_reduced = [utt for utt in utts for dia in dias_reduced if dia + '_utt' in utt]\n",
    "\n",
    "assert sorted(list(set([utt.split('_utt')[0] for utt in utts_reduced]))) == sorted(dias_reduced)\n",
    "\n",
    "print(f\"{len(utts_reduced)} utterances in the smaller-dataset\")\n",
    "\n",
    "shutil.rmtree('./smaller-dataset', ignore_errors=True)\n",
    "os.makedirs('./smaller-dataset')\n",
    "\n",
    "for row in annotations[1:]:\n",
    "    SrNo, Utterance, Speaker, Emotion, Sentiment, Dialogue_ID,\\\n",
    "        Utterance_ID, Season, Episode, StartTime, EndTime = row\n",
    "        \n",
    "    if f\"dia{Dialogue_ID}_utt\" not in str(utts_reduced):\n",
    "        continue\n",
    "\n",
    "    vid_full_path_src = os.path.join(vid_dir, f\"dia{Dialogue_ID}_utt{Utterance_ID}.mp4\")\n",
    "    vid_full_path_dst = os.path.join('./smaller-dataset/', os.path.basename(vid_full_path_src))\n",
    "\n",
    "    shutil.copyfile(vid_full_path_src, vid_full_path_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "actual number of videos copied: 789\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "actual_utts = sorted(glob('./smaller-dataset/*.mp4'))\n",
    "print(f\"actual number of videos copied: {len(actual_utts)}\")\n",
    "\n",
    "dias = sorted(set([path.split('_utt')[0] for path in actual_utts]))\n",
    "\n",
    "dias_LARGE = dias[:len(dias)]\n",
    "dias_MEDIUM = dias_LARGE[:len(dias_LARGE)//2]\n",
    "dias_SMALL = dias_LARGE[:len(dias_LARGE)//4]\n",
    "\n",
    "assert len(dias_LARGE) == LARGE\n",
    "assert len(dias_MEDIUM) == MEDIUM\n",
    "assert len(dias_SMALL) == SMALL\n",
    "\n",
    "for datasize, datasize_ in zip([SMALL, MEDIUM, LARGE], ['small', 'medium', 'large']):\n",
    "    dias_ = dias[:datasize]\n",
    "    train, dev, test = int(len(dias_)*0.8), int(len(dias_)*0.9), int(len(dias_)*1.0)\n",
    "\n",
    "    dias_train = dias_[:train]\n",
    "    dias_dev = dias[train:dev]\n",
    "    dias_test = dias[dev:test]\n",
    "\n",
    "    assert len(dias_train) + len(dias_dev) + len(dias_test) == datasize\n",
    "\n",
    "    utts_train = [os.path.basename(utt) for utt in actual_utts for dia in dias_train if dia + '_utt' in utt]\n",
    "    utts_dev = [os.path.basename(utt) for utt in actual_utts for dia in dias_dev if dia + '_utt' in utt]\n",
    "    utts_test = [os.path.basename(utt) for utt in actual_utts for dia in dias_test if dia + '_utt' in utt]\n",
    "\n",
    "    with open(f\"dataset-{datasize_}.json\", 'w') as stream:\n",
    "        to_dump = {'train': utts_train, 'dev': utts_dev, 'test': utts_test}\n",
    "        json.dump(to_dump, stream)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "for datasize_ in ['small', 'medium', 'large']:\n",
    "    with open(f\"dataset-{datasize_}.json\", 'r') as stream:\n",
    "        datasets[datasize_] = json.load(stream)\n",
    "\n",
    "assert set(datasets['small']['train'] + datasets['small']['dev'] + datasets['small']['test']).issubset(set(datasets['medium']['train']))\n",
    "\n",
    "\n",
    "assert set(datasets['medium']['train'] + datasets['medium']['dev'] + datasets['medium']['test']).issubset(set(datasets['large']['train']))"
   ]
  }
 ]
}